{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import enchant\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import enchant\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv(\"../data/parsed_news_articles.csv\")\n",
    "df['Text'] = df['Text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    d = enchant.Dict(\"en_GB\")\n",
    "    spcl= '[-,;@_!#$%^&*()<>?/\\|}{~:''.+\"\"]'\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens=[]\n",
    "    for token in tokens:\n",
    "        token = re.sub(spcl,'',token)\n",
    "        token = re.sub('[0-9]','',token)\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            token = token.lower()\n",
    "            if (token!='') and (d.check(token)):\n",
    "                filtered_tokens.append(token)\n",
    "    v = [word for word in filtered_tokens if word not in stop_words]\n",
    "    stems = [lemmatizer.lemmatize(t) for t in v]\n",
    "    temp = ' '.join(i for i in stems)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedArticles=[]\n",
    "\n",
    "for i in df.index:\n",
    "    c=preprocessor(df['Text'][i])\n",
    "    cleanedArticles.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "713"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleanedArticles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cleaned_text']=cleanedArticles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_articles = pd.read_csv(\"../data/parsed_test_articles.csv\")\n",
    "cleanedTestArticles=[]\n",
    "\n",
    "for i in test_articles.index:\n",
    "    c=preprocessor(test_articles['Text'][i])\n",
    "    cleanedTestArticles.append(c)\n",
    "    \n",
    "test_articles['Cleaned_text']=cleanedTestArticles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sim_search(query, datai):\n",
    "    cos_sim=[]\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    dataMatrixAll = []\n",
    "    similarity_dict=dict()\n",
    "    for i in range(len(query)):\n",
    "        query_i = query[i]\n",
    "        data = [query[i]]+cleanedArticles\n",
    "        lk=test_articles['Links'][i]\n",
    "        similarity_dict[lk]=list()\n",
    "        data = [query[i]]+datai\n",
    "        dataMatrix = {}\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(data)\n",
    "        vals = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)\n",
    "        for j in range(len(data)-1):\n",
    "            try:\n",
    "                dataMatrix[df.loc[df['Cleaned_text'] == datai[j], 'Links'].iloc[0]]=vals[j+1]\n",
    "            except Exception:\n",
    "                print(j)\n",
    "        \n",
    "        key_list = list(dataMatrix.keys())\n",
    "        val_list = list(dataMatrix.values())\n",
    "        sorted_vals = sorted(vals,reverse=True)[1:6]\n",
    "        for i in sorted_vals:\n",
    "            temp=key_list[val_list.index(i)]\n",
    "            similarity_dict[lk].append(temp)\n",
    "    return similarity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_scores(text):\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "    sentiment_dict = sid_obj.polarity_scores(text)\n",
    "    return sentiment_dict['compound']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiment\"]=\"\"\n",
    "for i in range(len(cleanedArticles)):\n",
    "    article_title = df['Title'].iloc[i]\n",
    "    article_body = cleanedArticles[i]\n",
    "    df['sentiment'][i] = sentiment_scores(article_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Links</th>\n",
       "      <th>Cleaned_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>The Indian Express</td>\n",
       "      <td>Comprehensive employment, industrial policies ...</td>\n",
       "      <td>since the employment and unemployment figures ...</td>\n",
       "      <td>https://indianexpress.com/article/opinion/colu...</td>\n",
       "      <td>since employment unemployment figure always es...</td>\n",
       "      <td>0.847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>The Indian Express</td>\n",
       "      <td>Explained: Is the push towards organised manuf...</td>\n",
       "      <td>For a while now, it has been suggested that In...</td>\n",
       "      <td>https://indianexpress.com/article/explained/ex...</td>\n",
       "      <td>suggested unemployment woe solved boosting man...</td>\n",
       "      <td>0.9956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>The Indian Express</td>\n",
       "      <td>It’s a wage crisis</td>\n",
       "      <td>At a deeper level, our acceptance of self-expl...</td>\n",
       "      <td>https://indianexpress.com/article/opinion/colu...</td>\n",
       "      <td>deeper level acceptance form mental bondage de...</td>\n",
       "      <td>0.9959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>The Indian Express</td>\n",
       "      <td>The rising insecurity</td>\n",
       "      <td>Much of the labour force shifting out of agric...</td>\n",
       "      <td>https://indianexpress.com/article/opinion/indi...</td>\n",
       "      <td>much labour force shifting agriculture found w...</td>\n",
       "      <td>0.9771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>The Indian Express</td>\n",
       "      <td>With India facing a job crisis, we should have...</td>\n",
       "      <td>Amazon to invest USD 1 bn in digitising Indian...</td>\n",
       "      <td>https://indianexpress.com/article/opinion/colu...</td>\n",
       "      <td>amazon invest digitising file amazon invest di...</td>\n",
       "      <td>0.7922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Source                                              Title  \\\n",
       "0  The Indian Express  Comprehensive employment, industrial policies ...   \n",
       "1  The Indian Express  Explained: Is the push towards organised manuf...   \n",
       "2  The Indian Express                                 It’s a wage crisis   \n",
       "3  The Indian Express                              The rising insecurity   \n",
       "4  The Indian Express  With India facing a job crisis, we should have...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  since the employment and unemployment figures ...   \n",
       "1  For a while now, it has been suggested that In...   \n",
       "2  At a deeper level, our acceptance of self-expl...   \n",
       "3  Much of the labour force shifting out of agric...   \n",
       "4  Amazon to invest USD 1 bn in digitising Indian...   \n",
       "\n",
       "                                               Links  \\\n",
       "0  https://indianexpress.com/article/opinion/colu...   \n",
       "1  https://indianexpress.com/article/explained/ex...   \n",
       "2  https://indianexpress.com/article/opinion/colu...   \n",
       "3  https://indianexpress.com/article/opinion/indi...   \n",
       "4  https://indianexpress.com/article/opinion/colu...   \n",
       "\n",
       "                                        Cleaned_text sentiment  \n",
       "0  since employment unemployment figure always es...     0.847  \n",
       "1  suggested unemployment woe solved boosting man...    0.9956  \n",
       "2  deeper level acceptance form mental bondage de...    0.9959  \n",
       "3  much labour force shifting agriculture found w...    0.9771  \n",
       "4  amazon invest digitising file amazon invest di...    0.7922  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Indian Express\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-b18168387824>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mke\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mke\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mcomparison_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "d=dict()\n",
    "keys = df['Source'].unique()\n",
    "for key in keys:\n",
    "    d[key]=list()\n",
    "for i in range(len(df)):\n",
    "    d[df['Source'][i]].append(df['Cleaned_text'][i])\n",
    "comparison_dict=dict()\n",
    "links = list(test_articles['Links'])\n",
    "for i in range (len(test_articles)):\n",
    "    comparison_dict[links[i]]=list()\n",
    "for source,articles in d.items():\n",
    "    print(source)\n",
    "    d2 = vectorize_sim_search(cleanedTestArticles,articles)\n",
    "    ke=list(d2.keys())\n",
    "    for key in ke:\n",
    "        comparison_dict[key].append(d2[key][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(comparison_dict.keys())\n",
    "sourcesl = list(d.keys())\n",
    "avg_sc = [0]*27\n",
    "for link in k:\n",
    "    for i in range(len(comparison_dict[link])):\n",
    "        avg_sc[i]=avg_sc[i]+df.loc[df['Links'] == comparison_dict[link][i], 'sentiment'].iloc[0]\n",
    "for i in range(len(avg_sc)):\n",
    "    avg_sc[i]=avg_sc[i]/29\n",
    "bias_list=[0]*27\n",
    "for i in range(len(avg_sc)):\n",
    "    if(avg_sc[i]<-0.05):\n",
    "        bias_list[i]='Left'\n",
    "    elif(avg_sc[i]>0.05):\n",
    "        bias_list[i]='Right'\n",
    "    else:\n",
    "        bias_list[i]='Center',\n",
    "bias_df = pd.DataFrame(list(zip(sourcesl, bias_list)),columns =['Source', 'Bias'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
