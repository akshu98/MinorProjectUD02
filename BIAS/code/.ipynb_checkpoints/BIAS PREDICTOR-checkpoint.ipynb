{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import enchant\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "import enchant\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv(\"../data/parsed_news_articles.csv\")\n",
    "df['Text'] = df['Text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    d = enchant.Dict(\"en_GB\")\n",
    "    spcl= '[-,;@_!#$%^&*()<>?/\\|}{~:''.+\"\"]'\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens=[]\n",
    "    for token in tokens:\n",
    "        token = re.sub(spcl,'',token)\n",
    "        token = re.sub('[0-9]','',token)\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            token = token.lower()\n",
    "            if token in spcl_names:\n",
    "                filtered_tokens.append(token)\n",
    "            if (token!='') and (d.check(token)):\n",
    "                filtered_tokens.append(token)\n",
    "    v = [word for word in filtered_tokens if word not in stop_words]\n",
    "    stems = [lemmatizer.lemmatize(t) for t in v]\n",
    "    temp = ' '.join(i for i in stems)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedArticles=[]\n",
    "\n",
    "for i in df.index:\n",
    "    c=preprocessor(df['Text'][i])\n",
    "    cleanedArticles.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleanedArticles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cleaned_text']=cleanedArticles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sim_search(query, datai):\n",
    "    cos_sim=[]\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    dataMatrixAll = []\n",
    "    similarity_dict=dict()\n",
    "    for i in range(len(query)):\n",
    "        query_i = query[i]\n",
    "        data = [query[i]]+cleanedArticles\n",
    "        lk=test_df['Links'][i]\n",
    "        similarity_dict[lk]=list()\n",
    "        data = [query[i]]+datai\n",
    "        dataMatrix = {}\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(data)\n",
    "        vals = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)\n",
    "        for j in range(len(data)-1):\n",
    "            dataMatrix[df.loc[df['Cleaned_text'] == datai[j], 'Links'].iloc[0]]=vals[j+1]\n",
    "        \n",
    "        key_list = list(dataMatrix.keys())\n",
    "        val_list = list(dataMatrix.values())\n",
    "        sorted_vals = sorted(vals,reverse=True)[1:6]\n",
    "        for i in sorted_vals:\n",
    "            temp=key_list[val_list.index(i)]\n",
    "            similarity_dict[lk].append(temp)\n",
    "    return similarity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_scores(text):\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "    sentiment_dict = sid_obj.polarity_scores(text)\n",
    "    return sentiment_dict['compound']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiment\"]=\"\"\n",
    "for i in range(len(cleanedArticles)):\n",
    "    article_title = df['Title'].iloc[i]\n",
    "    article_body = cleanedArticles[i]\n",
    "    df['sentiment'][i] = sentiment_scores(article_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=dict()\n",
    "keys = df['Source'].unique()\n",
    "for key in keys:\n",
    "    d[key]=list()\n",
    "for i in range(len(df)):\n",
    "    d[df['Source'][i]].append(df['Cleaned_text'][i])\n",
    "comparison_dict=dict()\n",
    "links = list(test_df['Links'])\n",
    "for i in range (len(test_df)):\n",
    "    comparison_dict[links[i]]=list()\n",
    "for source,articles in d.items():\n",
    "    print(source)\n",
    "    d2 = vectorize_sim_search(cleanedTestArticles,articles)\n",
    "    ke=list(d2.keys())\\n\",\n",
    "    for key in ke:\n",
    "        comparison_dict[key].append(d2[key][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(comparison_dict.keys())\n",
    "sourcesl = list(d.keys())\n",
    "avg_sc = [0]*27\n",
    "for link in k:\n",
    "    for i in range(len(comparison_dict[link])):\n",
    "        avg_sc[i]=avg_sc[i]+df.loc[df['Links'] == comparison_dict[link][i], 'sentiment'].iloc[0]\n",
    "for i in range(len(avg_sc)):\n",
    "    avg_sc[i]=avg_sc[i]/29\n",
    "bias_list=[0]*27\n",
    "for i in range(len(avg_sc)):\n",
    "    if(avg_sc[i]<-0.05):\n",
    "        bias_list[i]='Left'\n",
    "    elif(avg_sc[i]>0.05):\n",
    "        bias_list[i]='Right'\n",
    "    else:\n",
    "        bias_list[i]='Center'\\n\",\n",
    "bias_df = pd.DataFrame(list(zip(sourcesl, bias_list)),columns =['Source', 'Bias'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
