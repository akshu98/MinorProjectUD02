{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import wordnet \n",
    "from collections import Counter,OrderedDict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import enchant\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing news articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://indianexpress.com/article/opinion/columns/unemployment-jobs-india-plfs-survey-6106758/\n",
      "https://indianexpress.com/article/explained/expalined-is-the-push-towards-organised-manufacturing-the-answer-to-indias-jobs-crisis-6241308/\n",
      "https://indianexpress.com/article/opinion/columns/unemployment-india-job-data-gdp-5584683/\n",
      "https://indianexpress.com/article/opinion/india-job-crisis-mgnrega-economic-advisory-council-unemployment-the-rising-insecurity-6095152/\n",
      "https://indianexpress.com/article/opinion/columns/fifth-column-why-offend-mr-jeff-bezos-amazon-6223735/\n",
      "https://indianexpress.com/article/opinion/columns/the-problem-of-skilling-india-unemployment-joblessness-modi-government-5973808/\n",
      "https://indianexpress.com/article/opinion/columns/narendra-modi-bjp-government-indian-economy-india-jobs-unemployment-rate-niti-aayog-5757306/\n",
      "https://indianexpress.com/article/business/economy/six-year-low-and-slow-gdp-growth-recorded-at-4-5-per-cent-lowest-in-more-than-6-years-6143474/\n",
      "https://indianexpress.com/article/business/indias-urban-unemployment-rate-drops-to-9-3-per-cent-in-jan-mar-2019-govt-data-6133715/\n",
      "https://indianexpress.com/article/india/youth-unemployment-rising-with-educational-qualifications-study-6098754/\n",
      "https://indianexpress.com/elections/unemployment-rate-highest-in-2019-do-political-parties-really-care-have-a-look-at-manifesto-decision-2019-5674201/\n",
      "https://indianexpress.com/article/india/youth-felt-lack-jobs-new-panel-focus-gujarat-chief-secretary-jn-singh-5768898/\n",
      "https://indianexpress.com/article/india/sena-nehru-gandhi-family-cant-be-blamed-for-non-generation-of-jobs-5763396/\n",
      "https://indianexpress.com/article/cities/ahmedabad/gujarat-youth-jobless-bjp-govt-failed-on-job-issue-claims-congress-vijay-rupani-5484907/\n",
      "https://indianexpress.com/article/opinion/columns/slowdown-jammu-kashmir-congress-article-370-forgotten-the-flagging-economy-513506/\n",
      "https://indianexpress.com/article/jobs/national-male-workforce-shrinking-says-labour-report-that-govt-buried-nsso-job-report-5634838/\n",
      "https://indianexpress.com/article/india/prime-minister-modi-speech-jobs-employment-lok-sabha-no-confidence-motion-5268709/\n",
      "https://indianexpress.com/article/opinion/columns/development-unemployment-agrarian-crisis-policy-making-growth-across-the-aisle-governments-hide-people-seek-5211182/\n",
      "https://indianexpress.com/article/india/congress-to-focus-on-agriculture-employment-to-challenge-bjp-5085612/\n",
      "https://thewire.in/politics/modi-government-does-not-acknowledge-the-word-slowdown-manmohan-singh\n",
      "https://thewire.in/economy/2020-modi-first-priority-india-flailing-economic-ship\n",
      "https://thewire.in/economy/facing-the-reality-how-can-modi-reverse-the-current-economic-slowdown\n",
      "https://thewire.in/politics/opposition-reaction-budget-2020\n",
      "https://thewire.in/economy/budget-2020-india-economy\n",
      "https://thewire.in/economy/india-economic-slowdown-reconsider-cash-transfers\n",
      "https://thewire.in/economy/india-economy-rickety-bridge-over-troubled-waters\n",
      "https://thewire.in/health/icds-anganwadi-workers-helpers-midday-meals-minimum-wage-protest\n",
      "https://thewire.in/economy/india-employment-structural-transformation\n",
      "https://thewire.in/economy/budget-2020-youth-jobs\n",
      "https://thewire.in/economy/india-politics-economics-crisis-of-confidence-mistrust\n",
      "https://thewire.in/labour/9-million-jobs-lost-in-6-years-a-first-in-indian-history\n",
      "https://thewire.in/labour/centre-said-it-was-bringing-jobs-to-jk-instead-it-has-brought-more-unemployment\n",
      "https://thewire.in/education/on-manmohan-singhs-birthday-congress-renews-unemployment-charge-against-bjp-govt\n",
      "https://thewire.in/labour/santosh-gangwar-labour-minister-unemployment-india\n",
      "https://thewire.in/labour/centre-said-it-was-bringing-jobs-to-jk-instead-it-has-brought-more-unemployment\n",
      "https://thewire.in/government/congress-leaders-amp-pressure-on-govt-over-economy\n",
      "https://thewire.in/banking/shadow-banking-crisis-ilfs-dhfl\n",
      "https://thewire.in/rights/kashmir-article-370-leh-ladakh-land-job-reservation\n",
      "https://thewire.in/economy/goldman-sachs-sees-more-pain-in-store-for-the-indian-economy\n",
      "https://thewire.in/uncategorised/bengal-left-student-activists-police-clash-at-protest-against-unemployment\n",
      "https://thewire.in/economy/indias-economy-suffers-car-crash-pain-spreads-to-villages\n",
      "https://thewire.in/macro/manmohan-singh-india-economy-gdp\n"
     ]
    }
   ],
   "source": [
    "# f = open(\"news articles/articles.txt\",\"r\")\n",
    "# LOA = [line.rstrip() for line in f]\n",
    "# df = pd.DataFrame(columns = ['Title','Text','URL','Cleaned_text'])\n",
    "# for url in LOA: \n",
    "#     try:\n",
    "#         news_article = Article(url, language=\"en\")     \n",
    "#         news_article.download() \n",
    "#         news_article.parse() \n",
    "#         news_article.nlp() \n",
    "#         df2={'Title':news_article.title,'Text':news_article.text,'URL':url}\n",
    "#         df=df.append(df2,ignore_index=True)\n",
    "    \n",
    "#     except Exception:\n",
    "#         print(url)\n",
    "        \n",
    "        \n",
    "#df.to_csv(\"news articles/parsed_news_articles.csv\",index=None)\n",
    "\n",
    "data = pd.read_csv(\"Main.csv\")\n",
    "df= pd.DataFrame(data)\n",
    "#print(file)\n",
    "#df.head()\n",
    "\n",
    "links= df[\"Links\"]\n",
    "#(len(links))\n",
    "#links\n",
    "#df1.isnull().values.any()\n",
    "#df.dropna(subset = [\"Links\"], inplace=True)\n",
    "titles=[]\n",
    "text=[]\n",
    "for url in links:\n",
    "    try:\n",
    "   # print(url)\n",
    "        news_article = Article(url,encoding='utf-8')\n",
    "        news_article.download() \n",
    "        news_article.parse() \n",
    "        news_article.nlp()\n",
    "        titles.append(news_article.title)\n",
    "        text.append(news_article.text)\n",
    "        df[\"Title\"]=titles\n",
    "        df[\"Text\"]=text\n",
    "    \n",
    "    except Exception:\n",
    "        print(url)\n",
    "        \n",
    "        \n",
    "\n",
    "df\n",
    "   # df2=pd.DataFrame({'Title':news_article.title})\n",
    "    #df1[\"Title\"]=df1[\"Title\"].append(pd.DataFrame(news_article.title), ignore_index=True)\n",
    "#df2.head()\n",
    "\n",
    "df.to_csv(\"news articles/parsed_news_articles.csv\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the news articles into a dataframe\n",
    "To avoid parsing articles again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv(\"news articles/parsed_news_articles_copy.csv\")\n",
    "df['Text'] = df['Text'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    sentences=sent_tokenize(text)\n",
    "    d = enchant.Dict(\"en_US\")\n",
    "    spcl= '[-,;@_!#$%^&*()<>?/\\|}{~:''.+\"\"]'\n",
    "    c=[]\n",
    "    for i in sentences:\n",
    "        english_words=[]\n",
    "        i = i.lower()\n",
    "        i = re.sub(r'[^\\x00-\\x7F]+',' ', i)\n",
    "        i = re.sub('[0-9]',' ',i)\n",
    "        i = re.sub(spcl,' ',i)\n",
    "        i = re.sub('\\n\\n',' ',i)\n",
    "        i = re.sub('\\s\\s',' ',i)\n",
    "        v = i.split(' ')\n",
    "        v = [word for word in v if word not in stop_words]\n",
    "        v = [i for i in v if i!='']\n",
    "        [english_words.append(j) for j in v if d.check(j)]\n",
    "        temp=' '.join(word for word in english_words)\n",
    "        c.append(temp)\n",
    "    c=' '.join(sent for sent in c)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedArticles=[]\n",
    "\n",
    "for i in df.index:\n",
    "    text=df['Text'][i]\n",
    "    c=preprocessor(text)\n",
    "    cleanedArticles.append(c)\n",
    "cleanedArticles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding cleaned news articles to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cleaned_text']=cleanedArticles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the test articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "f = open(\"news articles/test_articles.txt\",\"r\")\n",
    "LOA = [line.rstrip() for line in f]\n",
    "test_df = pd.DataFrame(columns = ['Title','Text','Links','Cleaned_text'])\n",
    "for url in LOA: \n",
    "    news_article = Article(url, language=\"en\")     \n",
    "    news_article.download() \n",
    "    news_article.parse() \n",
    "    news_article.nlp() \n",
    "    df2={'Title':news_article.title,'Text':news_article.text}\n",
    "    test_df=test_df.append(df2,ignore_index=True)\n",
    "\n",
    "test_df.to_csv(\"news articles/parsed_test_articles.csv\",encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "cleanedTestArticles=[]\n",
    "for i,j in test_df.iterrows():\n",
    "    text=j['Text']\n",
    "    c=preprocessor(text)\n",
    "    cleanedTestArticles.append(c)\n",
    "    \n",
    "cleanedTestArticles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to find cosine similarity\n",
    "Finds top 5 similar articles for a given test article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sim_search(query, data):\n",
    "    #Inserting query in list position 0\n",
    "    cos_sim=[]\n",
    "    #data = query + cleanedArticles\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    dataMatrixAll = []\n",
    "  \n",
    "    for i in range(len(query)):\n",
    "        query_i = query[i]\n",
    "        print(\"MAIN ARTICLE:\\n\",query_i[0:len(query_i)//20],\"....\",\"\\n\\n\\n\")\n",
    "        print(\"RELATED ARTICLES:\\n\")\n",
    "        data = [query[i]]+cleanedArticles\n",
    "        dataMatrix = {}\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(data)\n",
    "        vals = cosine_similarity(tfidf_matrix[i], tfidf_matrix)[0]\n",
    "        \n",
    "        for j in range(len(data)-1):\n",
    "            dataMatrix[df['Links'][j]]=vals[j+1]\n",
    "        key_list = list(dataMatrix.keys()) \n",
    "        val_list = list(dataMatrix.values()) \n",
    "        sorted_vals = sorted(vals,reverse=True)[1:31]\n",
    "        for i in sorted_vals:\n",
    "            temp=key_list[val_list.index(i)]\n",
    "            print(temp,\"....\",\"\\tsimilarity score:\",i,\"\\n\")\n",
    "        print(\"-------------------------------------------------------------------------------------------\")\n",
    "        return sorted_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorize_sim_search(cleanedTestArticles,cleanedArticles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analyzer\n",
    "Here we are using VADER Sentiment Analysis. VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media, and works well on texts from other domains. <br><br>\n",
    "The compound score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive).<br>\n",
    "<br>\n",
    "positive sentiment: compound score >= 0.05 <br>\n",
    "neutral sentiment: (compound score > -0.05) and (compound score < 0.05) <br>\n",
    "negative sentiment: compound score <= -0.05 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_scores(text): \n",
    "  \n",
    "    # Create a SentimentIntensityAnalyzer object. \n",
    "    sid_obj = SentimentIntensityAnalyzer() \n",
    "  \n",
    "    # polarity_scores method of SentimentIntensityAnalyzer \n",
    "    # oject gives a sentiment dictionary. \n",
    "    # which contains pos, neg, neu, and compound scores. \n",
    "    sentiment_dict = sid_obj.polarity_scores(text) \n",
    "      \n",
    "    print(\"Overall sentiment dictionary is : \", sentiment_dict) \n",
    "    print(\"text was rated as \", sentiment_dict['neg']*100, \"% Negative\") \n",
    "    print(\"text was rated as \", sentiment_dict['neu']*100, \"% Neutral\") \n",
    "    print(\"text was rated as \", sentiment_dict['pos']*100, \"% Positive\") \n",
    "  \n",
    "    print(\"text Overall Rated As\", end = \" \") \n",
    "  \n",
    "    # decide sentiment as positive, negative and neutral \n",
    "    if sentiment_dict['compound'] >= 0.05 : \n",
    "        print(\"Positive\") \n",
    "  \n",
    "    elif sentiment_dict['compound'] <= - 0.05 : \n",
    "        print(\"Negative\") \n",
    "  \n",
    "    else : \n",
    "        print(\"Neutral\") \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cleanedArticles)):\n",
    "    article_title = df['Title'].iloc[i]\n",
    "    article_body = cleanedArticles[i]\n",
    "    #print(\"Article title:\",article_title,\"\\n\")\n",
    "    #sentiment_scores(article_title)\n",
    "    print(\"Article body:\",article_body,\"\\n\")\n",
    "    sentiment_scores(article_body)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying biased language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(source_df))\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "source_df = pd.read_csv(\"main.csv\")\n",
    "\n",
    "source_df.head()\n",
    "\n",
    "d=dict()\n",
    "\n",
    "keys = source_df['Source'].unique() \n",
    "\n",
    "for key in keys:\n",
    "    d[key]=list()\n",
    "\n",
    "df['Source']=source_df['Links']\n",
    "\n",
    "for i in range(len(source_df)):\n",
    "    d[source_df['Source'][i]].append(df['Cleaned_text'][i])\n",
    "    \n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
